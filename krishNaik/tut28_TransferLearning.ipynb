{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tut28_TransferLearning.ipynb",
      "provenance": [],
      "mount_file_id": "1YYzov7CWtKjejDkLQWfBU4zcex-yE40M",
      "authorship_tag": "ABX9TyN9e3fxB3zgV8NPQZ4LQIaA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatimazain118/Deep-Learning-Assignments/blob/main/krishNaik/tut28_TransferLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the algorithms/model designed for image classification are Xception, VGG16, VGG19, ResNet etc These have come up better accuracy when we deal thousands different categories of data.\n",
        "\n",
        "Using these model by the application of keras, in our model by changing only the output layer to solve our problem statement.This is what Transfer Learning is.\n",
        "\n",
        "**Transfer learning** is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task."
      ],
      "metadata": {
        "id": "o3OEsk4yMXvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Article to read](https://towardsdatascience.com/transfer-learning-with-vgg16-and-keras-50ea161580b4)"
      ],
      "metadata": {
        "id": "XZx6TfxpQZSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To import the Classification model like VGG16 based on CNN we import these Input, dense, flatten layers. \n",
        "\n",
        "In this model will be using only 4 categories in the output by appending in the last layer of VGG16 model/architecture, and thats what the transfer Learning does using the power of VGG16 in our model to solve our problem.\n",
        "\n",
        "**VGG16** is a convolution neural net (CNN ) architecture which was used to win ILSVR(Imagenet) competition in 2014. It is considered to be one of the excellent vision model architecture till date. ... It follows this arrangement of convolution and max pool layers consistently throughout the whole architecture.\n",
        "\n",
        "**ImageNet** is a large database or dataset of over 14 million images, and 1000 categorical output.\n"
      ],
      "metadata": {
        "id": "Qp5qPuTrRDol"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DZqFdr_pIYki"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input, Lambda, Dense, Flatten\n",
        "from keras.models import Model\n",
        "from keras.applications.vgg16 import VGG16      #importing VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator  #ImageDataGenerator helps in data Augumentation\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# re-size all the images to this as because VGG16 is designed in a way that it takes this size images only\n",
        "IMAGE_SIZE = [224, 224]"
      ],
      "metadata": {
        "id": "oYNCNZsuIsRp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Efe_vuDciOLK",
        "outputId": "580a6547-93fa-4ac2-f324-9bcaa7b209d4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = '/content/gdrive/MyDrive/Datasets/Train'\n",
        "valid_path = '/content/gdrive/MyDrive/Datasets/Test'"
      ],
      "metadata": {
        "id": "8w2GDhEiUX7p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add preprocessing layer to the front of VGG\n",
        "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
        "#adding coz of RGB has 3 channel if using grey images add 1\n",
        "# include_top = False means last layer of VGG16 we are not taking"
      ],
      "metadata": {
        "id": "UX7RlW0EUadb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0cbd23d-c872-4271-98cb-4c7be8fa70d6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# don't train existing weights as it's pre-trained model\n",
        "for layer in vgg.layers:\n",
        "  layer.trainable = False"
      ],
      "metadata": {
        "id": "H1KeBfv8UeLs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # useful for getting number of classes\n",
        "folders = glob('/content/gdrive/MyDrive/Datasets/Train/*')"
      ],
      "metadata": {
        "id": "oqQ5Ta54Ugdy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adding last layers - you can add more if you want\n",
        "x = Flatten()(vgg.output)\n",
        "# x = Dense(1000, activation='relu')(x)\n",
        "prediction = Dense(len(folders), activation='softmax')(x)"
      ],
      "metadata": {
        "id": "b5mZDRNvUmFG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a model object\n",
        "model = Model(inputs=vgg.input, outputs=prediction)"
      ],
      "metadata": {
        "id": "1LUCT5v3Zq8H"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view the structure of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4YCVJNDZ4P1",
        "outputId": "24d3b16e-274a-4b5e-b1ed-434dfc7c12f5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4)                 100356    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,815,044\n",
            "Trainable params: 100,356\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tell the model what cost and optimization method to use\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "YewpL0h1Z6cN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)"
      ],
      "metadata": {
        "id": "3tXMqLTVgubJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = train_datagen.flow_from_directory('/content/gdrive/MyDrive/Datasets/Train',\n",
        "                                                 target_size = (224, 224),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('/content/gdrive/MyDrive/Datasets/Test',\n",
        "                                            target_size = (224, 224),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'categorical')\n",
        "\n",
        "'''r=model.fit_generator(training_set,\n",
        "                         samples_per_epoch = 8000,\n",
        "                         nb_epoch = 5,\n",
        "                         validation_data = test_set,\n",
        "                         nb_val_samples = 2000)'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "_HKmxuQfgypA",
        "outputId": "18990ed2-a92a-4d6d-991e-6b6654993aa8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 images belonging to 0 classes.\n",
            "Found 0 images belonging to 0 classes.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'r=model.fit_generator(training_set,\\n                         samples_per_epoch = 8000,\\n                         nb_epoch = 5,\\n                         validation_data = test_set,\\n                         nb_val_samples = 2000)'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the model\n",
        "r = model.fit_generator(\n",
        "  training_set,\n",
        "  validation_data=test_set,\n",
        "  epochs=5,\n",
        "  steps_per_epoch=len(training_set),\n",
        "  validation_steps=len(test_set)\n",
        ")\n",
        "# loss\n",
        "plt.plot(r.history['loss'], label='train loss')\n",
        "plt.plot(r.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('LossVal_loss') \n",
        "\n",
        "# accuracies\n",
        "plt.plot(r.history['acc'], label='train acc')\n",
        "plt.plot(r.history['val_acc'], label='val acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('AccVal_acc')\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "model.save('facefeatures_new_model.h5')"
      ],
      "metadata": {
        "id": "F7mLnETCyDPg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}